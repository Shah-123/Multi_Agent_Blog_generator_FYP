import re
from langgraph.types import Send
from langchain_core.messages import SystemMessage, HumanMessage

from Graph.state import State, Plan, Task, EvidenceItem
from Graph.templates import WORKER_SYSTEM
from .utils import logger, llm_quality, _job, _emit

def fanout(state: State):
    """Generates parallel workers for each section."""
    if not state.get("plan"):
        logger.warning("No plan found, skipping fanout.")
        return []
    
    _emit(_job(state), "writer", "started", f"Dispatching {len(state['plan'].tasks)} parallel writers...")
    
    return [
        Send("worker", {
            "task": task.model_dump(),
            "topic": state["topic"],
            "mode": state["mode"],
            "plan": state["plan"].model_dump(),
            "evidence": [e.model_dump() for e in state.get("evidence", [])],
            "_job_id": state.get("_job_id", ""),
        })
        for task in state["plan"].tasks
    ]

def worker_node(payload: dict) -> dict:
    task = Task(**payload["task"])
    plan = Plan(**payload["plan"])
    evidence = [EvidenceItem(**e) for e in payload.get("evidence", [])]
    job_id = payload.get("_job_id", "")
    
    _emit(job_id, "writer", "working", f"Writing section {task.id + 1}/{len(plan.tasks)}: {task.title}", {"section": task.id + 1, "total": len(plan.tasks)})
    logger.info(f"‚úçÔ∏è Writing Section {task.id + 1}/{len(plan.tasks)}: {task.title} (Tone: {plan.tone})")

    try:
        bullets_text = "\n- " + "\n- ".join(task.bullets)
        evidence_text = "\n".join(
            f"- [{e.title}]({e.url}) ({e.published_at or 'Unknown Date'})\n  Content: {e.snippet[:300]}"
            for e in evidence[:15]
        )
        
        section_keywords = task.tags[:3]
        keywords_str = ", ".join(section_keywords) if section_keywords else "general topic"

        response = llm_quality.invoke(
            [
                SystemMessage(content=WORKER_SYSTEM.format(
                    tone=plan.tone,
                    keywords=keywords_str,
                    target_words=task.target_words
                )),
                HumanMessage(content=(
                    f"Blog Title: {plan.blog_title}\n"
                    f"Section Number: {task.id + 1} of {len(plan.tasks)}\n"
                    f"Section Title: {task.title}\n"
                    f"Goal: {task.goal}\n"
                    f"Target Words: {task.target_words}\n"
                    f"Tone: {plan.tone} (MAINTAIN THIS TONE CONSISTENTLY)\n"
                    f"Keywords to integrate naturally: {keywords_str}\n"
                    f"Bullets to Cover:{bullets_text}\n\n"
                    f"Available Evidence (Cite these URLs):\n{evidence_text}\n\n"
                    f"CRITICAL INSTRUCTIONS:\n"
                    f"1. Write EXACTLY {task.target_words} words (minimum)\n"
                    f"2. Cover ALL bullet points completely\n"
                    f"3. End with a complete sentence (period/question mark/exclamation)\n"
                    f"4. DO NOT stop mid-sentence or mid-paragraph\n\n"
                    f"Remember: Write in {plan.tone} tone throughout."
                )),
            ],
            max_tokens=3000
        )
        
        section_md = response.content.strip()
        
        lines = section_md.split('\n')
        if lines and re.match(r'^#{1,4}\s+', lines[0]):
            lines = lines[1:]
            section_md = '\n'.join(lines).strip()
        section_md = f"## {task.title}\n\n{section_md}"
        
        word_count = len(section_md.split())
        if word_count < (task.target_words * 0.7):
            logger.warning(f"Section {task.id + 1} seems short ({word_count} words, target: {task.target_words})")
        
        if not section_md.endswith(('.', '!', '?', '"', ')')):
            logger.warning(f"Section {task.id + 1} incomplete (doesn't end with punctuation)")
            section_md += "."
        
        logger.info(f"‚úÖ Completed: {word_count} words")
        _emit(job_id, "writer", "working", f"Completed section {task.id + 1}: {task.title} ({word_count} words)", {"section": task.id + 1, "words": word_count})
        
    except Exception as e:
        import traceback
        logger.error(f"Error in section {task.title}: {e}")
        traceback.print_exc()
        section_md = f"## {task.title}\n\n[Error generating content: {str(e)}]"
        _emit(job_id, "writer", "error", f"Failed section {task.id + 1}: {str(e)}")

    return {"sections": [(task.id, section_md)]}

def merge_content(state: State) -> dict:
    _emit(_job(state), "merger", "started", "Merging all sections into final blog...")
    logger.info("üîó MERGING SECTIONS ---")
    plan = state["plan"]
    
    unique_sections = {}
    for task_id, content in state["sections"]:
        unique_sections[task_id] = content
        
    ordered_content = [unique_sections[k] for k in sorted(unique_sections.keys())]
    
    body = "\n\n".join(ordered_content).strip()
    merged_md = f"# {plan.blog_title}\n\n{body}\n"
    
    word_count = len(merged_md.split())
    logger.info(f"‚úÖ Merged {len(ordered_content)} sections")
    _emit(_job(state), "merger", "completed", f"Merged {len(ordered_content)} sections ({word_count} words)", {"sections": len(ordered_content), "words": word_count})
    _emit(_job(state), "writer", "completed", f"All {len(ordered_content)} sections written", {"sections": len(ordered_content), "words": word_count})
    
    return {"merged_md": merged_md}
